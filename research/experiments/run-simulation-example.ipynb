{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b205de5b",
   "metadata": {},
   "source": [
    "## Example for Running Simulations outside UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1ed87",
   "metadata": {},
   "source": [
    "**Prerequisites: Install Backend code as library**\n",
    "\n",
    "Go to `sweng25_group22_multiagentsimframework/backend` and run `pip install -e .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b547e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e1c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from engine.simulation import SelectorGCSimulation\n",
    "from OLD.util.config import SimConfigLoader\n",
    "# from autogen_agentchat.agents import AssistantAgent\n",
    "# from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "# from autogen_agentchat.teams import SelectorGroupChat\n",
    "# from autogen_agentchat.ui import Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c27db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"config\": {\n",
    "    \"agents\": [\n",
    "      {\n",
    "        \"description\": \"An agent representing a buyer interested in purchasing a house\",\n",
    "        \"name\": \"Buyer\",\n",
    "        \"prompt\": \"You are the Buyer. Your goal is to purchase the house at the lowest possible price while achieving a satisfactory agreement.\"\n",
    "      },\n",
    "      {\n",
    "        \"description\": \"An agent representing the seller of a house\",\n",
    "        \"name\": \"Seller\",\n",
    "        \"prompt\": \"You are the Seller. Your goal is to sell the house at the highest possible price while ensuring a successful transaction.\"\n",
    "      },\n",
    "      {\n",
    "        \"description\": \"An agent facilitating negotiation to help reach a successful deal\",\n",
    "        \"name\": \"Mediator\",\n",
    "        \"prompt\": \"You are the Mediator. Your goal is to assist both Buyer and Seller in reaching a fair and successful negotiation.\"\n",
    "      }\n",
    "    ],\n",
    "    \"name\": \"Business Negotiation Simulation\",\n",
    "    \"output_variables\": [\n",
    "      {\n",
    "        \"name\": \"Final Price in USD\",\n",
    "        \"type\": \"Number\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Deal Reached\",\n",
    "        \"type\": \"Boolean\"\n",
    "      }\n",
    "    ],\n",
    "    \"termination_condition\": \"The simulation ends when a deal is reached or negotiations fail.\"\n",
    "  },\n",
    "  \"num_runs\": 10,\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b79bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-18 17:23:36,697 utils INFO: Using Azure OpenAI client for simulation.\n",
      "2025-06-18 17:23:36,708 engine.simulation INFO: Initializing SelectorGCSimulation with config: {'agents': [{'description': 'An agent representing a buyer interested in purchasing a house', 'name': 'Buyer', 'prompt': 'You are the Buyer. Your goal is to purchase the house at the lowest possible price while achieving a satisfactory agreement.'}, {'description': 'An agent representing the seller of a house', 'name': 'Seller', 'prompt': 'You are the Seller. Your goal is to sell the house at the highest possible price while ensuring a successful transaction.'}, {'description': 'An agent facilitating negotiation to help reach a successful deal', 'name': 'Mediator', 'prompt': 'You are the Mediator. Your goal is to assist both Buyer and Seller in reaching a fair and successful negotiation.'}], 'name': 'Business Negotiation Simulation', 'output_variables': [{'name': 'Final Price in USD', 'type': 'Number'}, {'name': 'Deal Reached', 'type': 'Boolean'}], 'termination_condition': 'The simulation ends when a deal is reached or negotiations fail.'}\n"
     ]
    }
   ],
   "source": [
    "simulation = SelectorGCSimulation(config['config'], max_messages=25, min_messages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1ed954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- SelectSpeakerEvent (SelectorGroupChatManager) ----------\n",
      "['Buyer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_selector_group_chat.py:270: UserWarning: Resolved model mismatch: gpt-4o-2024-08-06 != o3-mini-2025-01-31. Model mapping in autogen_ext.models.openai may be incorrect. Set the model to o3-mini-2025-01-31 to enhance token/cost estimation and suppress this warning.\n",
      "  response = await self._model_client.create(messages=select_speaker_messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (Buyer) ----------\n",
      "Hello there. I’ve looked over the listing, and I’m very interested in the house. I’d like to discuss what it would take to make this work for both of us. Based on my research and some concerns about needed updates and repairs, I’m hoping we can agree on a number that’s a bit more in line with my current budget.\n",
      "\n",
      "Before I make a formal offer, could you share a little more detail about the property’s recent maintenance, any repairs that might be needed, or any other factors that might influence the value? Once I have a better understanding, I’d be comfortable starting our negotiation at a price that reflects both the market and the work I might need to invest in the home.\n",
      "\n",
      "I’m confident that we can reach a satisfactory agreement, and I’m eager to hear your thoughts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:981: UserWarning: Resolved model mismatch: gpt-4o-2024-08-06 != o3-mini-2025-01-31. Model mapping in autogen_ext.models.openai may be incorrect. Set the model to o3-mini-2025-01-31 to enhance token/cost estimation and suppress this warning.\n",
      "  model_result = await model_client.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- SelectSpeakerEvent (SelectorGroupChatManager) ----------\n",
      "['Seller']\n",
      "---------- TextMessage (Seller) ----------\n",
      "Thank you for reaching out and for your interest in the property. I completely understand wanting all the details before moving forward, and I’m happy to share more about the major maintenance and upkeep we've done.\n",
      "\n",
      "Over the past few years, we’ve taken proactive steps to ensure the house remains in excellent condition. For example, the roof was completely replaced three years ago, and the HVAC system was upgraded just over two years ago. We’ve also had the exterior painted last year and refreshed the landscaping to keep the curb appeal high. Inside, the kitchen appliances and fixtures were updated fairly recently, so they’re still in peak condition.\n",
      "\n",
      "There are a few cosmetic items that you might consider updating eventually if you’d like to tailor the space more to your taste, but there are no major repairs looming. Regular preventative maintenance has been a priority, and the home has been well cared for overall. We even have the results of a recent inspection available if you’d like a closer look at the details.\n",
      "\n",
      "I’m confident that these updates and the solid history of maintenance underscore the value of the property. Of course, I’m open to discussing any concerns and looking at the numbers together to see how we can craft an agreement that works for both of us. What are your thoughts on the updates, and do you have any particular areas you’d like more details on?\n",
      "---------- SelectSpeakerEvent (SelectorGroupChatManager) ----------\n",
      "['Buyer']\n",
      "---------- TextMessage (Buyer) ----------\n",
      "Thanks for the detailed rundown—I really appreciate knowing the full history and the care that's been put into the property. It’s reassuring to see that major systems and features, like the roof, HVAC, and kitchen updates, have been handled so well. \n",
      "\n",
      "That said, while these updates definitely add value and peace of mind, I do have some reservations about the cosmetic elements that I might want to personalize down the road. I’ve done some research on similar homes in the area, and after factoring in potential updates, I’m leaning toward an initial offer that’s a bit more conservative than the current listing. \n",
      "\n",
      "Before I put a number on the table, would it be possible to review the recent inspection report you mentioned? I just want to confirm there aren’t any hidden details that might affect my decision. Once I have that in hand, I’d feel comfortable moving forward with an offer that reflects both the great maintenance record and the work I’d eventually want to invest in the home’s personalization. \n",
      "\n",
      "What are your thoughts on sharing the inspection details, and is there any flexibility you might anticipate in the pricing based on a full picture of the property?\n",
      "---------- SelectSpeakerEvent (SelectorGroupChatManager) ----------\n",
      "['Seller']\n",
      "---------- TextMessage (Seller) ----------\n",
      "I completely understand your desire to review all the details before moving forward. I can certainly share a copy of the recent inspection report so you can see firsthand that there are no hidden issues. The report provides a detailed account of the inspections, and I’m confident you’ll see that while there are some cosmetic items that could be updated, everything else meets high standards given the ongoing care and proactive maintenance the home has received.\n",
      "\n",
      "Regarding pricing, while the property reflects the quality of the updates and maintenance we’ve invested in, I do understand that every buyer has a budget and vision for personalization. I’m open to discussing the numbers once you’ve had a chance to review the inspection report. I believe that once you see the full picture, you’ll find that the asking price is a strong reflection of the value and upkeep of the home. \n",
      "\n",
      "Let me know the best way to send you the inspection report, and we can go from there. I'm confident we can work out a deal that respects both your budget and the quality of the property.\n",
      "---------- SelectSpeakerEvent (SelectorGroupChatManager) ----------\n",
      "['Buyer']\n",
      "---------- TextMessage (Buyer) ----------\n",
      "Thanks for the quick response—I appreciate the willingness to share the inspection report. Could you please send me a digital copy (PDF, if possible) of the report to my email at buyer@example.com? I’d like to review it in detail so I can fully understand the current condition and confirm there are no unexpected issues.\n",
      "\n",
      "Once I’ve had a chance to go through it, I’d be happy to talk numbers. While your updates and maintenance history are impressive, I’m still factoring in some cosmetic updates I’d eventually like to make. That said, I’m confident that after reviewing the report, we can discuss a price that reflects both the excellent condition of the major systems and the extra work I may plan for personalization.\n",
      "\n",
      "Looking forward to your email and moving ahead from there. Thanks again!\n",
      "---------- SelectSpeakerEvent (SelectorGroupChatManager) ----------\n",
      "['Seller']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for Seller_33dec8c3-6bf6-4d42-8511-b9290a8ad90c/33dec8c3-6bf6-4d42-8511-b9290a8ad90c\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 852, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 981, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 624, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/_base_client.py\", line 1762, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Error processing publish message for Buyer_33dec8c3-6bf6-4d42-8511-b9290a8ad90c/33dec8c3-6bf6-4d42-8511-b9290a8ad90c\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for Mediator_33dec8c3-6bf6-4d42-8511-b9290a8ad90c/33dec8c3-6bf6-4d42-8511-b9290a8ad90c\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for InformationReturnAgent_33dec8c3-6bf6-4d42-8511-b9290a8ad90c/33dec8c3-6bf6-4d42-8511-b9290a8ad90c\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\nTraceback:\nTraceback (most recent call last):\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 852, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 981, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/_base_client.py\", line 1762, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m simulation_result = \u001b[38;5;28;01mawait\u001b[39;00m simulation.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/multi-agent-social-simulation/research/sweng25_group22_multiagentsimframework/backend/src/engine/simulation.py:100\u001b[39m, in \u001b[36mSelectorGCSimulation.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     simulation_results = \u001b[38;5;28;01mawait\u001b[39;00m Console(\u001b[38;5;28mself\u001b[39m.group_chat.run_stream())\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_result(simulation_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:518\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    515\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    516\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    519\u001b[39m     stop_reason = message.message.content\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\nTraceback:\nTraceback (most recent call last):\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 852, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 981, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/_base_client.py\", line 1762, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/demiangholipour/anaconda3/envs/sweng/lib/python3.11/site-packages/openai/_base_client.py\", line 1562, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n"
     ]
    }
   ],
   "source": [
    "simulation_result = await simulation.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
